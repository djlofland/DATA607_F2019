---
title: 'DATA607 Assignment 9'
subtitle: 'NYT - Web APIs'
author: 'Donny Lofland'
data: '10/26/2019'
output:
  html_document:
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float: true
  pdf_document: defaults
---

# SETUP

Source files: [https://github.com/djlofland/DATA607_F2019/tree/master/Week9](https://github.com/djlofland/DATA607_F2019/tree/master/Week9)

```{r setup, eval=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(forcats)
library(jsonlite)
library(formattable)

# We will be working with the NYT Movies API's
NYT_MOVIE_API_BASE <- "https://api.nytimes.com/svc/movies/v2"
NYT_ACTION_PICKS <- "/reviews/picks.json?"

NYT_ACTION_CRITIC <- "/critics/A.%20O.%20Scott.json?"
NYT_ACTION_CRITICS <- "/critics/all.json?"

NYT_ACTION_SEARCH <- "/reviews/search.json?"

# Note that my NYT API Key is stored in my .Renviron file making it available as an envioronmental variable in any projects.
APIKEY <- paste("&api-key=", Sys.getenv("NYTIMES_APIKEY_D607"), "&", sep="")

# Local cache data file to use if available
local_cache_fn <- './data/reviews.csv'
```

# Load Critics

We will be working with the NYT Movies API's

```{r}
url <- paste(NYT_MOVIE_API_BASE, NYT_ACTION_CRITICS, APIKEY, sep="")
print(url)

critics <- fromJSON(url, flatten = TRUE) %>% 
  data.frame() 
```

# Load Movie Picks

We will be working with the NYT Movies API's

```{r}
url <- paste(NYT_MOVIE_API_BASE, NYT_ACTION_PICKS, APIKEY, sep="")
print(url)

movie_picks <- fromJSON(url, flatten = TRUE) %>% 
  data.frame() 
```

# Load Movie Reviews

We will be working with the NYT Movies API's.  Note that the NYUT API has throttlelimits that only allow 6 queries/min and a maximum of 4000 queries per day.  In this example, I setup a loop to see if I could scrape all the Movie Reviews.  After several attempts, its clear that if I really want **all** movie reviews, I'll have to babysit the process for the better part of a day.  So, I allowed it to run 
758 times at 20 reviews pare page for a total of 15160 rows (or movie reviews).  This is more than sufficient for proof of concept in this project.

```{r load_reviews}
# Check if we have a local copy of the data available to load
isCacheFound <- FALSE

# Check whether we previously downloaded the review data from NYT API.  If found, we will used the cached copy.  If not found, then we'll kick off job to pull the data from NYT (note: this takes a while)
if(file.exists(local_cache_fn)) {
  msg <- paste('Found cached copy: ', local_cache_fn, sep='')
  isCacheFound <- TRUE
}

if(!isCacheFound) {
  # Kick of process to download data from NYT API
  
  # List that will hold each page of results returned from the API
  reviews <- list()
  
  # We will need to check the has_more column 
  has_more <- TRUE
  page <- 0
    
  while(has_more) {
    # construct the URL - note we add an offset to handle 20 results per page
    # in the respone, if has_more column is TRUE, we will need to request the next page
    url <- paste(NYT_MOVIE_API_BASE, NYT_ACTION_SEARCH, APIKEY, "offset=", page*20, sep="")
    
    # download results from API
    results <- fromJSON(url, flatten = TRUE) %>% 
      data.frame() 
    
    message("Retrieving page: ", page + 1)
    
    # save off the current page of results
    reviews[[page+1]] <- results
    
    # scrape whether we have more results we'll need to pull
    has_more <- results$has_more[1]
    page <- page + 1
    
    # Throttle we can use to only grab the first few pages of results
    #  if(page==3) {
    #    has_more <- FALSE
    #  }
    
    # NYT API has a rate limmit of 10 requests per minute (HTTP error 429 if this is exceeded)
    Sys.sleep(6.1)
  }
  
  # combine the list of dataframes into a single main dataframe
  data_df <- data.table::rbindlist(reviews) 
  
  # Cache the processed as CSV for future
  write.csv(data_df, local_cache_fn, row.names=FALSE, na="")
  
} else {
  # Now load the data from local cache CSV
  data_df <- read_csv(local_cache_fn, col_names = TRUE)
  isDataLoaded <- TRUE
  msg <- 'Cached CSV data loaded.'
}
```

## Cleanup Reviews  

```{r analysis}
# drop rows with missing dates - since I'm not really interested in dates and am not
# doing any analysis where loss of rows might affect interpretation, it's safe to just
# drop rows with any missing dates.  Note: There are also nulls in the rating column 
# for movies that didn't receive an official mpaa rating letter.  
# For these, I'll replace nulls with 'not rated'.  Next, teh ByLines are a mess - I'll do some
# quick spot cleanup.
data_df2 <- data_df %>% 
  drop_na(results.publication_date, results.opening_date, results.date_updated) %>%
  replace_na(list(results.mpaa_rating = "Not Rated")) %>%
  mutate(results.byline = str_to_upper(results.byline)) %>%
  mutate(results.byline = str_replace_all(results.byline, "[\\.,]", "")) %>%
  mutate(results.byline = str_replace_all(results.byline, "&#160;", "")) %>%
  mutate(results.byline = str_replace_all(results.byline, "BY ", "")) %>%
  mutate(results.byline = str_replace_all(results.byline, "&NBSP;", "")) %>%
  mutate(results.byline = str_trim(results.byline, side=c("both"))) %>%
  mutate(results.byline = str_replace_all(results.byline, "[;]", "")) %>%
  mutate(results.byline = str_squish(results.byline))
```

# Analysis

## Top 10 Posting Critics  

```{r top_10_critics}
# How many reviews were authored by critic - show the top 10 most prolific
critics_counts <- data_df2 %>%
  group_by(results.byline) %>%
  summarize(review_count = length(results.byline)) %>%
  arrange(desc(review_count)) %>%
  top_n(10)

formattable(critics_counts,
            align=c("l", "c"))
```

## Reviews by Critical & Rating  

```{r rating_by_critic}
# How many reviews did critics post for each mpaa rating?
rating_counts <- data_df2 %>%
  group_by(results.byline, results.mpaa_rating) %>%
  summarize(rating_count = length(results.mpaa_rating)) %>%
  arrange(results.byline) %>%
  spread(results.mpaa_rating, rating_count) %>%
  select(results.byline, G, PG, `PG-13`, R, `NC-17`, X, `Not Rated`) %>%
  replace_na(list(G = 0,PG=0, `PG-13`=0, R=0, `NC-17`=0, X=0, `Not Rated`=0))

formattable(rating_counts,
            align =c("l", "c", "c", "c", "c", "c", "c", "c"), 
            list(`results.byline` = formatter(
              "span", style = ~ style(color = "grey",font.weight = "bold")))
            )
```
