---
title: 'DATA607 Assignment 9'
subtitle: 'NYT - Web APIs'
author: 'Donny Lofland'
data: '10/26/2019'
output:
  html_document:
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float: true
  pdf_document: defaults
---

# SETUP

Source files: [https://github.com/djlofland/DATA607_F2019/tree/master/Week9](https://github.com/djlofland/DATA607_F2019/tree/master/Week9)

```{r setup, eval=TRUE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(forcats)
library(jsonlite)

# We will be working with the NYT Movies API's
NYT_MOVIE_API_BASE <- "https://api.nytimes.com/svc/movies/v2"
NYT_ACTION_PICKS <- "/reviews/picks.json?"

NYT_ACTION_CRITIC <- "/critics/A.%20O.%20Scott.json?"
NYT_ACTION_CRITICS <- "/critics/all.json?"

NYT_ACTION_SEARCH <- "/reviews/search.json?"

# Note that my NYT API Key is stored in my .Renviron file making it available as an envioronmental variable in any projects.
APIKEY <- paste("&api-key=", Sys.getenv("NYTIMES_APIKEY_D607"), "&", sep="")

# Local cache data file to use if available
local_cache_fn <- './data/reviews.csv'
```

# Load Critics

We will be working with the NYT Movies API's

```{r}
url <- paste(NYT_MOVIE_API_BASE, NYT_ACTION_CRITICS, APIKEY, sep="")
print(url)

critics <- fromJSON(url, flatten = TRUE) %>% 
  data.frame() 
```

# Load Movie Picks

We will be working with the NYT Movies API's

```{r}
url <- paste(NYT_MOVIE_API_BASE, NYT_ACTION_PICKS, APIKEY, sep="")
print(url)

movie_picks <- fromJSON(url, flatten = TRUE) %>% 
  data.frame() 
```

# Load Movie Reviews

We will be working with the NYT Movies API's

```{r load_reviews}
# Check if we have a local copy of the data available to load
isCacheFound <- FALSE

# Check whether we previously downloaded the review data from NYT API.  If found, we will used the cached copy.  If not found, then we'll kick off job to pull the data from NYT (note: this takes a while)
if(file.exists(local_cache_fn)) {
  msg <- paste('Found cached copy: ', local_cache_fn, sep='')
  isCacheFound <- TRUE
}


if(!isCacheFound) {
  # Kick of process to download data from NYT API
  
  # List that will hold each page of results returned from the API
  reviews <- list()
  
  # We will need to check the has_more column 
  has_more <- TRUE
  page <- 0
    
  while(has_more) {
    # construct the URL - note we add an offset to handle 20 results per page
    # in the respone, if has_more column is TRUE, we will need to request the next page
    url <- paste(NYT_MOVIE_API_BASE, NYT_ACTION_SEARCH, APIKEY, "offset=", page*20, sep="")
    
    # download results from API
    results <- fromJSON(url, flatten = TRUE) %>% 
      data.frame() 
    
    message("Retrieving page: ", page + 1)
    
    # save off the current page of results
    reviews[[page+1]] <- results
    
    # scrape whether we have more results we'll need to pull
    has_more <- results$has_more[1]
    page <- page + 1
    
    # Throttle we can use to only grab the first few pages of results
    #  if(page==3) {
    #    has_more <- FALSE
    #  }
    
    # NYT API has a rate limmit of 10 requests per minute (HTTP error 429 if this is exceeded)
    Sys.sleep(6.1)
  }
  
  # combine the list of dataframes into a single main dataframe
  data_df <- data.table::rbindlist(reviews) 
  
  # Cache the processed as CSV for future
  write.csv(data_df, local_cache_fn, row.names=FALSE, na="")
  
} else {
  # Now load the data from local cache CSV
  data_df <- read_csv(local_cache_fn, col_names = TRUE)
  isDataLoaded <- TRUE
  msg <- 'Cached CSV data loaded.'
}
```

## Cleanup Reviews  

```{r analysis}
# drop rows with missing dates
data_df2 <- data_df %>% 
  drop_na(results.publication_date, results.opening_date, results.date_updated)
```

# Analysis

## Top 10 Posting Critics  

```{r top_10_critics}
# drop rows with missing dates
critics_counts <- data_df2 %>%
  group_by(results.byline) %>%
  summarize(review_count = length(results.byline)) %>%
  arrange(desc(review_count)) %>%
  top_n(10)

critics_counts
```

